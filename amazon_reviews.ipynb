{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Amazon Reviews Competition\n\n## Big Data Analytics\n\n#### Group 13:\n\n*Vincent Ott <br>\nLisa Wittmann <br>\nSophia Lorenz*\n\n**October 5, 2023 <br>\nUniversity of Amsterdam**","metadata":{}},{"cell_type":"markdown","source":"## *Table of Contents*\n\n[1. Libraries](#section-one) <br>\n\n[2. Introduction](#section-two) <br>\n\n[3. Raw Data](#section-three) <br>\n\n[4. Data Preprocessing](#section-four) <br>\n\n[5. Training and Test Data Split](#section-five) <br>\n\n[6. Tokenization](#section-six) <br>\n\n[7. Feature Engineering](#section-seven) <br>\n\n[8. Redunant Variables](#section-eight) <br>\n\n[9. Sparse Matrices](#section-nine) <br>\n\n[10. Model Fitting](#section-ten) <br>\n- [Shrinkage Methods](#subsection-ten-one) <br>\n    - [Ridge Regression](#subsection-ten-one-one) <br>\n    - [Lasso](#subsection-ten-one-two) <br>\n- [Dimension Reduction Methods](#subsection-two-one) <br>\n    - [Principal Components Regression](#subsection-two-one-one) <br>\n    - [Partial Least Squarews](#subsection-two-one-two) <br>\n\n[11. Model Evaluation](#section-eleven) <br>\n\n[12. Model Comparison](#section-twelve) <br>\n\n[13. Predictions on Test Data](#section-thirteen) <br>\n\n[14. Submission](#section-fourteen) <br>\n\n[15. Discussion](#section-fivteen) <br>\n\n[16. References](#section-sixteen) <br>\n\n[17. Task Division](#section-seventeen) <br>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n\n\n## 1 Importing Packages","metadata":{}},{"cell_type":"code","source":"library(tidyverse)\nlibrary(tidytext)\nlibrary(doMC)\nlibrary(pls)","metadata":{"_uuid":"c91e583abe80b0537679954bd6c8ffa891d03d01","_execution_state":"idle","execution":{"iopub.status.busy":"2023-10-09T11:39:28.169978Z","iopub.execute_input":"2023-10-09T11:39:28.172657Z","iopub.status.idle":"2023-10-09T11:39:31.381346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n## 2 Introduction\n\nIn this competition we will predict customer satisfaction regarding [Baby products purchased on Amazon.com](http://jmcauley.ucsd.edu/data/amazon/), based on their written reviews. The probability that a customer is satisfied is defined as rating > 3. \n\n### 2.1 Task and Goal\n\nWe built binary classifier that recognize customer satisfaction from textual reviews of baby products. Reviews were collected by Amazon on their webpage with a maximum satisfaction score of 5. We will define each customer review larger than three stars as satisfied and every customer review scoring three or smaller as unsatisfied. The perfomance will be measuerd by evaluating the area under the curve (AUC) of the receiver operating curve (ROC).\n\n\n### 2.2 Dataset\n\nThe dataset origins from reviews on Amazon.com (http://jmcauley.ucsd.edu/data/amazon/). It contains 82.83 million unique reviews, from approximately 20 million users over a timespan of 19 years (May 1996 - 2014). The dataset contains 9.35 million items. Satisfaction was measured on on a 5-point Likert scale with 5 stars being greatly satisfied. To what extent will our models and results be generalizable? Two things are important to note. First, the data is quite outdated. It is from the very beginnings of e-commerce until 2014. Since 2014 it is also likely, that there have been major changes in e-commerce. For example, Amazon customers that ordered baby products in 2004 are probably a different demographic compared to customers in 2024. Especially if we consider that Amazon's core business used to be books. It was probably less common to order baby products from Amazon in 2004 than it will be in 2024. So, the language and satisfaction in Amazon customers might be different in 2024. The second aspect to consider about the generalizability is that the dataset only contains reviews for baby products. It is therefore unlikely that the models will hold for products such as razors or fantasy novels: \"These diapers just saved my day\" vs. \"I read non-stop for 10 hours\". In sum, our models and results should only generalize to a limited degree. Especially if we consider that there is more contemporary data readily available (which is outside the scope of the current competition).\n\n\n### 2.3 Candidate Machine Learning Models and Features\n\nThe classification task on hand deals with a binary classification problem - customers being satisfied vs. unsatisfied. Furthermore, we are working with text and therefore will face a lot of features. Therefore our predictions of customer satisfaction were based on the following models: \n\nWe used logistic regressions using shrinkage methods. Using shrinkage methods, i.e. constraining / regularizing the coefficient estimates such that the coefficient estimates are shrinked towards zero are beneficial as they result in sparser models (James et al., 2021). Thus, the following was applied:\n- Ridge Regression <br>\n- Lasso Regression <br>\n\nWe also applied models using dimension reduction methods. Reduction methods transform predictors and fit least squares models on the transformed variables. This way the regression problem is reduced from estimating all parameters p + 1 coefficients to linear combinations M + 1 predictors, resulting in reduced dimensions of the model (James et al., 2021). \n- Principal Component Regression <br>\n- Partial Least Squares <br>\n\nWe chose the following features to include in those models: \n- **document occurence**: We encoded each token as either being present (1) or being not present (0) in a document.\n- **token counts**: We created a document-by-term matrix which contains each token and its corresponding count within the document.\n- **term frequency**: We calculated the relative frequency of a term within a document\n- **inverse document frequency**: We calculated the inverse of the relative frequency with which a term occurs among all documents, expressed on a log scale (a measure of 'surprise')\n- the **product of TF and IDF**\n\n\n### 2.4 Bayes' error bound\n\nThe performance measure in this competition is AUC. On the other hand, the literature on sentiment analysis of Amazon reviews typically reports accuracy as a performance measure. However, accuracy and AUC are closely related and we will also not only look at the AUCs of our models but also at the accuracies. Thus, the accuracies serve as good indicators of lower Bayes' error bounds:\n\nHaque et al. (2018) used a TF-IDF approach like us and classified reviews in positive (>3) and negative (<3). The cross-validated accuracies with logistic regression reached from 81.99% to 91.43%. The highest accuracy came with a linear support vector machine at 94.02%.\n\nGiven the limited scope of our project, an accuracy of 80% to 90% would therefore already be a good performance. The Bayes' error bound, however, is at least 94.02%.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n## 3 Raw Data\n\nThe dataset consists of:\n- a dataset containing the reviews on Amazon regarding baby products. The dataframe contains both, the trainind data and test data. The test data are the reviews for which the rating is missing for which we will provide a prediction. \n- a test dataset containing the reviews on Amazon regarding baby products.\nThe data are stored in csv (.csv) files. ","metadata":{}},{"cell_type":"code","source":"# data attached to this notebook\nlist.files(path = \"../input\")","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:39:34.889766Z","iopub.execute_input":"2023-10-09T11:39:34.935180Z","iopub.status.idle":"2023-10-09T11:39:34.961804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# locate and load the data into memory\ndir(\"../input\", recursive = TRUE)","metadata":{"_uuid":"f66f7f950943394a8a4a148177f47055988d9967","execution":{"iopub.status.busy":"2023-10-09T11:39:37.273683Z","iopub.execute_input":"2023-10-09T11:39:37.276278Z","iopub.status.idle":"2023-10-09T11:39:37.323879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find the right file path\ncsv_filepath = dir(\"..\", pattern=\"amazon_baby.csv\", recursive=TRUE, full.names = TRUE)\n\n# read in the csv file\namazon = read_csv(csv_filepath) %>%\n    rownames_to_column('id') ","metadata":{"_uuid":"9ac5a75e758d075847213a02fe4ccff21c025b2a","execution":{"iopub.status.busy":"2023-10-09T11:39:39.374914Z","iopub.execute_input":"2023-10-09T11:39:39.377342Z","iopub.status.idle":"2023-10-09T11:39:42.200466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataframe contains the product `name`, the textual `review`, and the `rating`:","metadata":{}},{"cell_type":"code","source":"head(amazon)","metadata":{"_uuid":"d7e71a1371135eaec75ec363d8477f40d51868d4","execution":{"iopub.status.busy":"2023-10-09T11:39:42.205121Z","iopub.execute_input":"2023-10-09T11:39:42.206756Z","iopub.status.idle":"2023-10-09T11:39:42.358308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To get an idea how many products / datapoints each rating category has, we counted the ratings per rating category.","metadata":{}},{"cell_type":"code","source":"# count per rating category\namazon %>% group_by(rating) %>% summarize(n = n())","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:39:45.641664Z","iopub.execute_input":"2023-10-09T11:39:45.643327Z","iopub.status.idle":"2023-10-09T11:39:46.312913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The counts indicate that there are much fewer products / datapoints with low ratings as compared with high ratings. There are n = 300000 products / datapoints without a rating (`NA`). These serve as test data.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n## 4 Data Preprocessing\n\n### 4.1 Pasting `name`and `review` into a single string\n\nSince baby products differ on quality which will cause the ratings to differ, we included product identity as a predictive feature. To do so, we prepended the product name to the review text.\nBy not handling product names separately, we also have at least the name of the reviews that are empty strings and can predict the ratings of such reviews. ","metadata":{}},{"cell_type":"code","source":"# paste the `name` string and `review` string into a single string by \"–\"\namazon = amazon %>% \n    unite(review, name, review, sep = \" — \", remove = FALSE)\n\nhead(amazon)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:39:49.865873Z","iopub.execute_input":"2023-10-09T11:39:49.867772Z","iopub.status.idle":"2023-10-09T11:39:52.166941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Satisfaction Split\n\nThe aim of this competition is to predict the probability that a customer is ***satisfied***. This is deemed to be the case if `rating > 3`.  Hence, we need a dependent variable `y` a factor that specifies whether this is the case. Therefore, we added an extra column to the dataframe categorizing each review into satisfied (1) or unsatisfied (0). ","metadata":{}},{"cell_type":"code","source":"# Extra column to define staisfied vs. unsatisfied customers\n# binary problem for customer rating larger than 3\namazon <- amazon %>%\n  mutate(satisfied = case_when(\n    is.na(rating) ~ NA,       # When rating is NA, satisfied should be NA\n    rating <= 3 ~ 0,          # When rating is <= 3, satisfied should be 0\n    rating > 3 ~ 1            # When rating is > 3, satisfied should be 1\n  ))\nhead(amazon)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:39:55.302711Z","iopub.execute_input":"2023-10-09T11:39:55.304687Z","iopub.status.idle":"2023-10-09T11:39:55.386673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will need a dependent variable `y` (customer satisfaction) as a factor that specifies whether customers ratings is larger than 3. This dependent variable `y`, namely customer satisfaction got factorized.","metadata":{}},{"cell_type":"code","source":"# Factorize satisfied column\namazon$satisfied <- factor(amazon$satisfied)\nstr(amazon)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:39:59.019906Z","iopub.execute_input":"2023-10-09T11:39:59.021787Z","iopub.status.idle":"2023-10-09T11:39:59.200943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n## 5 Training and Test Data Split\n\nThe logical index variable was applied to select the desired rows without the need to split the data frame into seperate sets. This way feature extraction will become easier. Data was split by selecting datapoints containing `NA`'s in the `satisfied` column as test data for which we want to make predictions. The remaining datapoints containing data in the `satisfied` column serves as training data. There are 153.531 training samples and 30.000 test samples.","metadata":{}},{"cell_type":"code","source":"# count of training data and test data (contains `NA`s)\ntrainidx = !is.na(amazon$rating)\ntable(trainidx)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:40:01.982331Z","iopub.execute_input":"2023-10-09T11:40:01.984339Z","iopub.status.idle":"2023-10-09T11:40:02.066624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# count of training data\namazon_train = amazon %>% filter(trainidx)\namazon_train %>% nrow()\n\n# training dataframe\namazon_train %>% head(3)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:40:05.510786Z","iopub.execute_input":"2023-10-09T11:40:05.512683Z","iopub.status.idle":"2023-10-09T11:40:05.593922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# count of test data\namazon_test = amazon %>% filter(!trainidx)\namazon_test %>% nrow()\n\n# Number of NA's in `satisfied`\nsum(is.na(amazon$satisfied))\n\n# test dataframe\namazon_test %>% head(3)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:40:08.368985Z","iopub.execute_input":"2023-10-09T11:40:08.370831Z","iopub.status.idle":"2023-10-09T11:40:08.539011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test data contains `NA` values in the `rating`and `satisfied` column.","metadata":{}},{"cell_type":"markdown","source":"### 5.1 Sample Train Data from original Dataframe\n\nWe are working with a vast amount of data, so called big data. So, for working in this workbook, it is advisable to only work with a subset of`amazon_train`. This makes the computation time more feasible, especially when training and testing our features and the model fit. However, for the final predictions we sampled the full set of `amazon_train` and thus make use of all the available training data.","metadata":{}},{"cell_type":"code","source":"# set.seed(4)\n# start with smallish sample: e.g. 5000\n# for you final model use all train data: 153531\nsampled_amazon_train = amazon_train %>% sample_n(5000) \nsampled_amazon_train %>% nrow()","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:40:11.612325Z","iopub.execute_input":"2023-10-09T11:40:11.614447Z","iopub.status.idle":"2023-10-09T11:40:11.655682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Merge `sampled_amazon_train` and `amazon_test`\n\nWe merged again the sample training dataset and test dataset before creating features.","metadata":{}},{"cell_type":"code","source":"# merging the sample training set and test set\namazon_merged = rbind(sampled_amazon_train, amazon_test)\n\nhead(amazon_merged)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:40:14.601539Z","iopub.execute_input":"2023-10-09T11:40:14.603371Z","iopub.status.idle":"2023-10-09T11:40:14.677984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# counts of the training data and test data (contraining NA`s)\nmerged_trainidx = !is.na(amazon_merged$rating)\ntable(merged_trainidx)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:40:18.313718Z","iopub.execute_input":"2023-10-09T11:40:18.315774Z","iopub.status.idle":"2023-10-09T11:40:18.348040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be seen that the training dataset now only consists of a subsample with n = 5000 whereas the test data still consists of the full set of test data (n = 30000). For the final predictions we made use of the full training dataset consisting of n = 153531 datapoints.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-six\"></a>\n## 6 Tokenization\n\nWe used `tidytext` to break up the text into separate tokens and count the number of occurences per review. Tokens are single words (unigrams), pairs of words called bi-grams, or groups of words, n-grams. \n\n### 6.1 Unigrams\n\nUnigrams are single tokens (words) in the review text. They are the simplest and most basic form of text representation, where each token is considered as a separate unit. Unigrams are computationally efficient. We use the unigrams to prepare the text data for the following analysis. Further, used the unigrams to identify and analyze the sentiment or emotional tone of a piece of text.","metadata":{"_uuid":"6098328162f312fdc8dbc3928891e85dbf1e479a"}},{"cell_type":"code","source":"tokenize_unigrams = . %>% \n\n   # tokenize reviews at word level\n   unnest_tokens(token, review) %>%\n\n   # keep id (row number) in the result\n   # keep satisfied (predictor) in the result\n   # product names are included in the tokens\n   # count tokens within reviews as 'n'\n   count(id, satisfied, token)","metadata":{"_uuid":"fca425b0d51862febe3d304fe00b95b7d0cd0769","execution":{"iopub.status.busy":"2023-10-09T11:40:21.359457Z","iopub.execute_input":"2023-10-09T11:40:21.361427Z","iopub.status.idle":"2023-10-09T11:40:21.376877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a unigram\namazon_merged %>% head() %>% tokenize_unigrams() %>% head()","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:40:23.934928Z","iopub.execute_input":"2023-10-09T11:40:23.936777Z","iopub.status.idle":"2023-10-09T11:40:24.012355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen, in a unigram the token consists of a single word.","metadata":{}},{"cell_type":"markdown","source":"### 6.2 Bigrams\n\nBigrams are sequences of two consecutive tokens (words) that appear together in the review text. Bigrams consist of pairs of words that occur one after the other in a given order within a review. Therefore, they can provide insights into which pairs of words commonly co-occur in a text which is valuable information for capturing some level of context and relationship between words. We might use the bigrams for a sentiment analysis of the review texts.","metadata":{}},{"cell_type":"code","source":"tokenize_bigrams = . %>%\n\n   # tokenize reviews at word level\n   unnest_tokens(token, review, token = \"ngrams\", n = 2) %>%\n   \n   # keep id (row number) in the result\n   # keep satisfied (predictor) in the result\n   # the two product names are included in each token column\n   # count tokens within reviews as 'n'\n   count(id, satisfied, token)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:40:26.904304Z","iopub.execute_input":"2023-10-09T11:40:26.906362Z","iopub.status.idle":"2023-10-09T11:40:26.922109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a bigram\namazon_merged %>% head() %>% tokenize_bigrams() %>% head()","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:40:29.829283Z","iopub.execute_input":"2023-10-09T11:40:29.831223Z","iopub.status.idle":"2023-10-09T11:40:29.891009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen, in a bigram the token consists of two words.","metadata":{}},{"cell_type":"markdown","source":"### 6.3 Stopwords\n\nA problem of $TF_{d,t}$ is that it does not take into account that certain words simply occur more frequently because of their role in language (such as 'a', 'but', etc.). We removed such words that we theorized to be unimportant for the sentiment analysis.  Words such as 'is', 'a', 'the', etc, putatively do not carry a lot of information for the textual analysis and occur quite frequently in the review texts. Given that their usage was assumed to not vary a lot across satisfied vs. unsatisfied review texts, we decided to remove stopwords as defined in a customised stopwords list.\nWhether those potential features are indeed uninformative with respect to the target variable was tested statistically.\n\nAs stopwords do not make a big difference, we decided not to include them.","metadata":{"_uuid":"d837bb3b5f81da05a9012fb9f945603e2d0dc08c"}},{"cell_type":"code","source":"#stopwords = get_stopwords() \n#head(stopwords)\n\n# remove stopwords\n#amazon_merged_ <- \n #   amazon_merged_1 %>%\n    \n    # stopwords are applied per word\n  #  anti_join(stopwords, by = c(\"word\" = \"token\"))\n\n# peek at the result\n#head(amazon_merged_1)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T08:36:10.989417Z","iopub.execute_input":"2023-10-09T08:36:10.991144Z","iopub.status.idle":"2023-10-09T08:36:11.132081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-seven\"></a>\n## 7 Feature Engineering\n\nFeatures computed for tokens in text are based on the Bag of Words (BoW) model (a bag of Tokens). In this approach each document is considered a bag of words, in which order is disregarded.\n\nWe used the following token features in the model: \n\n- **document occurence**: We encoded each token as either being present (1) or being absent (0) in a document. The presence or absence of specific words or terms can help identify patterns, similarities, or differences between documents.\n- **token counts $n_{t,d}$**: We created a document-by-term matrix which contains each token $t$ and its corresponding count $n_{t,d}$ within a document $d$.\n- **term frequency ($TF_{d,t}$)**: We calculated the relative frequency of a term within a document $\\displaystyle {n_{d,t} \\over  \\sum_t n_{d,t}}$. The motivation for $TF_{d,t}$ is simply that the more often a token $t$ occurs in a document, the more likely it is that the topic of the document is closely related to that token. \n- **inverse document frequency $IDF_t$**: We calculated the inverse of the relative frequency with which a term occurs among all $N$ documents, expressed on a log scale (a measure of 'surprise') as $-\\log\\left({DF_t \\over N}\\right)$. Here $DF_t$ is the number of documents that contain the token $t$. The motivation for the $IDF_t$ is that the more wide spread the use of a token $t$ is among all documents, the less likely it conveys information about the topic of any particular document. Hence, the more surprising a word is, the more likely it conveys information about the topic of the document in which it is found. $IDF_t$ simply scales the $TF_{d,t}$ features accross documents. *This scaling may have an effect on scale sensitive algorithms like PCA and algorithms that rely on Euclidean distances such as kNN.*\n- the **product of term frequency TF and inverse document frequency IDF $TFIDF_{d,t}$**: This quantifies the importance of a term for a given document. \n- the **average word length**: This quantifies the average length of a word.\n- the **word count per review**: This quantifies the count of words per review assuming that unsatisfied reviews tend to have more words.\n- **Unigrams**: Unigrams are single tokens (words) in the review text. They are the simplest and most basic form of text representation, where each token is considered as a separate unit. Unigrams are computationally efficient.\n- **Bigrams**: Bigrams are sequences of two consecutive tokens (words) that appear together in the review text. Bigrams consist of pairs of words that occur one after the other in a given order within a review. Therefore, they can provide insights into which pairs of words commonly co-occur in a text which is valuable information for capturing some level of context and relationship between words.\n\nFor those features we created helper functions.","metadata":{"_uuid":"fdf9400f7df6154bc5376ce3c5a213f56dd0560d"}},{"cell_type":"markdown","source":"### 7.1 Document occurence\n\nDocument occurrence refers to a measure that indicates whether a word appears in a given review (document) within a collection of reviews (documents). Document occurrence is a binary measure, meaning it indicates whether the term is present (1) or absent (0) in a review (document). It is a basic form of term frequency representation in text analysis.","metadata":{}},{"cell_type":"code","source":"# document occurence\namazon_merged %>% tokenize_unigrams() %>% filter(n == 0) %>% head(3)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:11:25.449298Z","iopub.execute_input":"2023-10-09T11:11:25.450885Z","iopub.status.idle":"2023-10-09T11:11:59.381710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As of yet, for each id there are only tokens which also occur. Thus, there are no non-occuring tokens for each id.","metadata":{}},{"cell_type":"markdown","source":"### 7.2 Token counts\n\nThe token counts were already implemented in column `n`.","metadata":{}},{"cell_type":"markdown","source":"### 7.3 Term frequency $TF_{d,t}$, Inverse document frequency $IDF_t$, and Product $TFIDF_{d,t}$\n\nThis helper function was built to calculate term frequency $TF_{d,t}$, the inverse document frequency $IDF_t$, and the product of term frequency and inverse document frequency $TFIDF_{d,t}$.","metadata":{}},{"cell_type":"code","source":"# define a function using dplyr pipe\nto_tf_idf = . %>% \n\n    # compute TF·IDF for each word per sentence\n    bind_tf_idf(token, id, n) %>% \n\n    # words that are not present in a particular scentence are NA's but should be 0\n    replace_na(list(tf = 0, idf = Inf, tf_idf = 0))","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:43:08.066830Z","iopub.execute_input":"2023-10-09T11:43:08.069568Z","iopub.status.idle":"2023-10-09T11:43:08.089367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.4 Average word length in unigrams\n\nWe calculated the mean of word length across all reviews.","metadata":{}},{"cell_type":"code","source":"# Calculate average word length of unigram tokens and bind to a dataframe\navg_word_length <- . %>%\n  \n  # Calculate length of each word\n  mutate(word_length = nchar(token)) %>%\n  \n  # Group by ID and calculate average word length\n  group_by(id) %>%\n  summarise(avg_word_length = mean(word_length, na.rm = TRUE))","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:43:10.333408Z","iopub.execute_input":"2023-10-09T11:43:10.335237Z","iopub.status.idle":"2023-10-09T11:43:10.352300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.5 Word count per Review\n\nWe calculated the word count per review (`ID`). ","metadata":{}},{"cell_type":"code","source":"# Word count per review \nword_count <- . %>% \n\n  # group by review id\n  group_by(id) %>%\n\n  # count the number of words\n  summarise(word_count = sum(n()))","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:43:13.358641Z","iopub.execute_input":"2023-10-09T11:43:13.360583Z","iopub.status.idle":"2023-10-09T11:43:13.380673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.6 Unigrams\n\nUnigrams were created which contain the token, the token count, the term frequency `$TF_{d,t}$`, the inverse document frequency `$IDF_t$`, and the product of the two `$TFIDF_{d,t}$`, respectively. Further, we included the mean word length per review (`avg_word_length`) and the word count (`word_count`).","metadata":{}},{"cell_type":"code","source":"# create unigrams \nunigrams_df <- amazon_merged %>% tokenize_unigrams() %>% to_tf_idf()\n\n\n# Calculate average word length of unigram tokens and bind to unigrams_df\navg_word_length <- unigrams_df %>%\n  \n  # Calculate length of each word\n  mutate(word_length = nchar(token)) %>%\n  \n  # Group by ID and calculate average word length\n  group_by(id) %>%\n  summarise(avg_word_length = mean(word_length, na.rm = TRUE))\n\n\n# Bind avg_word_length to unigrams_df\nunigrams_df <- unigrams_df %>%\n  left_join(avg_word_length, by = \"id\")\n\n\n\n# Word count per review \nword_count <- unigrams_df %>% \n\n  # group by review id\n  group_by(id) %>%\n\n  # count the number of words\n  summarise(word_count = sum(n()))\n\n\n# Bind word_count to unigrams_df\nunigrams_df <- unigrams_df %>%\n  left_join(word_count, by = \"id\")\n\n\ntail(unigrams_df)\ndim(unigrams_df)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:43:16.473277Z","iopub.execute_input":"2023-10-09T11:43:16.475507Z","iopub.status.idle":"2023-10-09T11:43:56.262353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a safeguard, we use a boxplot to visualize the distribution of \"surprise\" that the tokens come with.","metadata":{}},{"cell_type":"code","source":"# boxplot of the inverse document frequency `idf` of the unigram data\nboxplot_idf <- unigrams_df %>%\n  ggplot(aes(x = 1, y = idf)) +  # Using x = 1 to create a single boxplot\n  geom_boxplot(fill = \"blue\") + \n  labs(title = \"Distribution of IDF (Inverse Document Frequency)\",\n       x = \"All reviews\", y = \"IDF (Inverse Document Frequency)\") + \n  theme(legend.position = \"none\") +\n  theme_minimal()\n\nboxplot_idf","metadata":{"execution":{"iopub.status.busy":"2023-10-09T09:32:44.595092Z","iopub.execute_input":"2023-10-09T09:32:44.597729Z","iopub.status.idle":"2023-10-09T09:32:49.214526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We should see that the vast majority of tokens lies between 1 and 4.\n\nNext, we created a boxplot to visualise the average word length distribution.","metadata":{}},{"cell_type":"code","source":"# boxplot of the average word length distribution\nplot_reviews_awl <- unigrams_df %>%\n  ggplot(aes(x = 1, y = avg_word_length)) +\n  geom_boxplot(fill = \"blue\") + \n  labs(title = \"Distribution of average word length\",\n       x = \"All reviews\", y = \"Average word length\") + \n  theme(legend.position = \"none\") +\n  theme_minimal()\n\nplot_reviews_awl","metadata":{"execution":{"iopub.status.busy":"2023-10-09T10:07:02.803443Z","iopub.execute_input":"2023-10-09T10:07:02.805065Z","iopub.status.idle":"2023-10-09T10:07:06.762662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We should see that the average word length is quite spread across amazon reviews with the most average word length value being between 4 and 5.\n\nFinally, we implemented a boxplot to visualize the word count.","metadata":{}},{"cell_type":"code","source":"# boxplot of the word_count distribution\nplot_word_count <- unigrams_df %>%\n  ggplot(aes(x = 1, y = word_count)) +\n  geom_boxplot(fill = \"blue\") + \n  labs(title = \"Distribution of word_count\",\n       x = \"All reviews\", y = \"Word count\") + \n  theme(legend.position = \"none\") +\n  theme_minimal()\n\nplot_word_count","metadata":{"execution":{"iopub.status.busy":"2023-10-09T10:07:57.819832Z","iopub.execute_input":"2023-10-09T10:07:57.821275Z","iopub.status.idle":"2023-10-09T10:08:04.040096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We should see that the word count is widely spread across amazon reviews with the majority of word counts being between 50 and 110 words.","metadata":{}},{"cell_type":"markdown","source":"### 7.7 Bigrams\n\nBigrams were created which contain the token, the token count, the term frequency `$TF_{d,t}$`, the inverse document frequency `$IDF_t$`, and the product of the two `$TFIDF_{d,t}$`, respectively. Further, we included the mean word length per review (`avg_word_length`) and the word count (`word_count`).","metadata":{}},{"cell_type":"code","source":"# create bigrams \nbigrams_df <- amazon_merged %>% tokenize_bigrams() %>% to_tf_idf()\n\n\n# Calculate average word length of bigram tokens and bind to bigrams_df\navg_word_length <- bigrams_df %>%\n  \n  # Calculate length of each word\n  mutate(word_length = nchar(token)) %>%\n  \n  # Group by ID and calculate average word length\n  group_by(id) %>%\n  summarise(avg_word_length = mean(word_length, na.rm = TRUE))\n\n\n# Bind avg_word_length to the bigrams_df dataframe\nbigrams_df <- bigrams_df %>%\n  left_join(avg_word_length, by = \"id\")\n\n\n# Word count per review \nword_count <- bigrams_df %>% \n\n  # group by review id\n  group_by(id) %>%\n\n  # count the number of words\n  summarise(word_count = sum(n()))\n\n\n# Bind word_count to bigrams_df \nbigrams_df <- bigrams_df %>%\n  left_join(word_count, by = \"id\")\n\n\ntail(bigrams_df)\ndim(bigrams_df)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:44:06.608408Z","iopub.execute_input":"2023-10-09T11:44:06.610364Z","iopub.status.idle":"2023-10-09T11:45:18.446055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We plotted the distribution of \"surprise\" that the tokens come with using the `idf`.","metadata":{}},{"cell_type":"code","source":"# boxplot of the inverse document frequency `idf` of the bigram data\nboxplot_idf <- bigrams_df %>%\n  ggplot(aes(x = 1, y = idf)) +  # Using x = 1 to create a single boxplot\n  geom_boxplot(fill = \"blue\") + \n  labs(title = \"Distribution of IDF (Inverse Document Frequency)\",\n       x = \"All reviews\", y = \"IDF (Inverse Document Frequency)\") + \n  theme(legend.position = \"none\") +\n  theme_minimal()\n\nboxplot_idf","metadata":{"execution":{"iopub.status.busy":"2023-10-09T09:33:03.574239Z","iopub.execute_input":"2023-10-09T09:33:03.575994Z","iopub.status.idle":"2023-10-09T09:33:06.222136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We should see that the vast majority of tokens has an IDF between 1 and 4.\n\nNext, we created a boxplot to visualise the average word length distribution.","metadata":{}},{"cell_type":"code","source":"# boxplot of the average word length distribution\nplot_reviews_awl <- bigrams_df %>%\n  ggplot(aes(x = 1, y = avg_word_length)) +\n  geom_boxplot(fill = \"blue\") + \n  labs(title = \"Distribution of average word length\",\n       x = \"All reviews\", y = \"Average word length\") + \n  theme(legend.position = \"none\") +\n  theme_minimal()\n\nplot_reviews_awl","metadata":{"execution":{"iopub.status.busy":"2023-10-09T10:15:07.852498Z","iopub.execute_input":"2023-10-09T10:15:07.854316Z","iopub.status.idle":"2023-10-09T10:15:12.190937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We should see that the average word length is quite spread across amazon reviews with the most average word length value being between 4 and 5.\n\nFinally, we implemented a boxplot to visualize the word count.","metadata":{}},{"cell_type":"code","source":"# boxplot of the word_count distribution\nplot_word_count <- bigrams_df %>%\n  ggplot(aes(x = 1, y = word_count)) +\n  geom_boxplot(fill = \"blue\") + \n  labs(title = \"Distribution of word_count\",\n       x = \"All reviews\", y = \"Word count\") + \n  theme(legend.position = \"none\") +\n  theme_minimal()\n\nplot_word_count","metadata":{"execution":{"iopub.status.busy":"2023-10-09T10:15:12.193314Z","iopub.execute_input":"2023-10-09T10:15:12.194561Z","iopub.status.idle":"2023-10-09T10:15:18.403410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We should see that the word count is widely spread across amazon reviews with the majority of word counts being between 50 and 110 words.","metadata":{}},{"cell_type":"markdown","source":"### 7.8 Sentiment Analyses\n\nWe used a helper function in order to implement the libraries for the sentiment analysis.","metadata":{}},{"cell_type":"code","source":"# helper function to get a lexicon library\nget_lexicon <- function(lexicon_name = names(textdata:::download_functions)) {\n    lexicon_name = match.arg(lexicon_name)\n    textdata:::download_functions[[lexicon_name]](\".\")\n    rds_filename = paste0(lexicon_name,\".rds\")\n    textdata:::process_functions[[lexicon_name]](\".\",rds_filename)\n    readr::read_rds(rds_filename)\n}","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:45:28.749148Z","iopub.execute_input":"2023-10-09T11:45:28.751042Z","iopub.status.idle":"2023-10-09T11:45:28.767502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 7.8.1 Sentiments with NRC library\n\nWe opted for the NRC Sentiment Vocabulary due to its comprehensive array of emotions for discerning the sentiment within a given review. This vocabulary encompasses a spectrum of emotional states, including anger, anticipation, disgust, fear, joy, sadness, surprise, confidence, and both negative and positive sentiments. The quantity of these identified emotions within a review should effectively convey the prevailing mood. Utilizing this diversity of emotions allows us to more precisely differentiate between negative and positive states, enhancing our ability to recognize various expressions such as frustration. ","metadata":{}},{"cell_type":"code","source":"# load NRC word library\nnrc <- get_lexicon('nrc')\n\n# NRC scores for unigrams\nnrc_df_uni <- unigrams_df %>%\n    inner_join(nrc, by = c(\"token\" = \"word\"), copy = TRUE, relationship = \"many-to-many\") %>%\n    group_by(id, sentiment) %>% \n    mutate(sentiment = ifelse(is.na(sentiment), \"nrc_\", paste0(\"nrc_\", sentiment)))\n\nhead(nrc_df_uni)\ndim(nrc_df_uni)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:45:32.020318Z","iopub.execute_input":"2023-10-09T11:45:32.022235Z","iopub.status.idle":"2023-10-09T11:45:37.728441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also aimed to apply the nrc on bigrams as we assumed better predictions of the sentiment analysis. However, this was computationally expensive and was therefore not implemented.","metadata":{}},{"cell_type":"code","source":"# NRC scores for bigrams\n# nrc_df_bi <- bigrams_df %>%\n  # separate(bigrams_df, into = c(\"word1\", \"word2\"), sep = \" \") %>%\n  # inner_join(unigrams_df, by = c(\"word1\" = \"word\"), copy = TRUE, relationship = \"many-to-many\") %>%\n  # group_by(id, sentiment) %>% \n  # mutate(sentiment = ifelse(is.na(sentiment), \"nrc_\", paste0(\"nrc_\", sentiment)))\n  \n# head(nrc_df_bi)\n# dim(nrc_df_bi)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T10:36:05.681989Z","iopub.execute_input":"2023-10-09T10:36:05.683583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 7.8.2 Sentiments with BING library\n\nAlternatively, the BING library which assigns 'positive' or 'negative' to a token of the unigram / bigram. We expected words associated with positive sentiments to occur frequently in positive reviews and such with negative sentiments in negative reviews.\n\nNote: code only works for unigrams. Would be interesting to see how predictions perform on bigrams. We assumed the predictions to get even better.","metadata":{}},{"cell_type":"code","source":"# load bing word library\nbing <- get_sentiments('bing') \n\n# assignment of negative \nbing_df_uni <- unigrams_df %>%\n    inner_join(bing, by = c(\"token\" = \"word\"), copy = TRUE, relationship = \"many-to-many\") %>%\n    group_by(id, sentiment) %>% \n    mutate(sentiment = ifelse(is.na(sentiment), \"bing_\", paste0(\"bing_\", sentiment)))\n\nhead(bing_df_uni)\ndim(bing_df_uni)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:48:55.041801Z","iopub.execute_input":"2023-10-09T11:48:55.044357Z","iopub.status.idle":"2023-10-09T11:48:56.923595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Features Dataframe","metadata":{}},{"cell_type":"markdown","source":"We combined the features into one dataframe. The dataframe contains the following features: \n- the token count\n- the term frequency `$TF_{d,t}$`\n- the inverse document frequency `$IDF_t$`\n- the product of the two `$TFIDF_{d,t}$`, respectively\n- mean word length per review (`avg_word_length`)\n- word count (`word_count`)\n\n\nFirst, we combined the unigrams with bigrams.","metadata":{}},{"cell_type":"code","source":"# combining the unigrams and bigrams\n# unigrams_bigrams_combined = rbind(nrc_df_uni, bing_df_uni)\n# features_df = unigrams_df %>% \n    # left_join(bigrams_df, by = \"id\", relationship = \"many-to-many\")\n\n# head(unigrams_bigrams_combined)\n\n# sum(nrow(unigrams_df), nrow(bigrams_df)) == nrow(unigrams_bigrams_combined)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:36:48.684810Z","iopub.execute_input":"2023-10-09T11:36:48.687070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately, we were not able to join the unigram containing sentiments with the bigrams containing sentiments due to high computational expenses.","metadata":{}},{"cell_type":"code","source":"# combining the unigrams and bigrams\n# nrc_bing_combined = rbind(nrc_df_uni, bing_df_uni)\n# nrc_bing_combined = nrc_df_uni %>% \n    # left_join(bing_df_uni, by = \"id\", relationship = \"many-to-many\")\n\n# head(nrc_bing_combined)\n# dim(nrc_bing_combined)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:53:43.626297Z","iopub.execute_input":"2023-10-09T11:53:43.628331Z","iopub.status.idle":"2023-10-09T11:53:43.787728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately, we were not able to join the unigram containing nrc sentiments with the unigrams containing bing sentiments due to high computational expenses.","metadata":{}},{"cell_type":"code","source":"# combining the unigrams with the bigrams both containing sentiments\n# features_df = rbind(nrc_df_uni, nrc_df_bi)\n# features_df = unigrams_df %>% \n    # left_join(bigrams_df, by = \"id\", relationship = \"many-to-many\") %>% \n\n# head(features_df)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T10:35:53.497273Z","iopub.execute_input":"2023-10-09T10:35:53.499104Z","iopub.status.idle":"2023-10-09T10:35:53.517633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We ended up using the unigrams_df upon which we trained our model. This model was computationally the least expensive while still containing valuable features.","metadata":{}},{"cell_type":"code","source":"features_df = unigrams_df","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:16:58.671765Z","iopub.execute_input":"2023-10-09T11:16:58.673851Z","iopub.status.idle":"2023-10-09T11:16:58.691275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-eight\"></a>\n## 8 Redunant Variables\n\nRedunant variables refer to variables which have: \n* Near-zero variance\n* High correlation\n* Multi-collinearity\n\nRedunant variables can be removed from the training dataset because they do not contribute meaningful information to the analysis and do not help discriminate between observations. By removing such variables, we simplify our model, which can lead to more interpretable results and understandable models. \n\nMoreover, having too many features or variables, especially those with low or no variance, can lead to overfitting. By removing uninformative variables, we reduce the risk of overfitting and improve the model's generalization to new data. \n\nFinally, redunant variables increase the computational cost of modeling and analysis without providing any real benefit. Removing them can speed up the processing time and reduce memory usage (James et al., 2021).\n\n### 8.1 Non-zero variance features\n\nVariables with near-zero variation are variables with mostly one and the same value. Variables with no or very little variation can be removed from the training dataset because they are essentially constant and do not help discriminate between observations.\n\nIn order to check for features with near zero variance, we decided not to use the `nearZeroVar` function of the `caret` package' because it is computationally ineffective.\n\nFor binary and count data as is the case here the variance is determined by the average. Hence, for the current data we accounted for document frequencies.\n\n\n### 8.2 Highly correlated features\n\nHigh correlation between features indicate they provide same or very similar information. This leads to instability of coefficient estimates (collinearity or inestimability in the worst cases). In order to correct for these highly correlated variables one would actually remove one of the two correlated features.\n\nAlthough correlated (linear combinations of) features may exist in this dataset, it would we computationally too cumbersome to remove them with those vasts amounts of data. Nevertheless, ridge regression and lasso inherently allow to disregard a specific measure to control for highly correlated features.\n\n\n### 8.3 Multicollinearity\n\nMulticollinearity refers to the situation where two or more variables are highly correlated, meaning that highly correlated linear combinations of features exist. In fact, those variables can be exactly predicted from other features. \n\nWe would technically have to control for multicollinearity but regularization techniques will do this inherently.\n\n\n### 8.4 Low frequency tokens\n\nWe theorized that tokens with a very low frequency were either idiosyncratic strings of miss-spellings that occur only in singular reviews. Therefore, tokens that occured in less than 0.01% of the documents were also disregarded in the model fitting. This allowed to remove tokens that appeared in less than 18 reviews (see below).\n\n$$IDF_t = -\\log\\left({\\text{df}_t \\over N}\\right) = -\\log(\\text{proportion of document in which }t\\text{ occurs})$$ \n\nwe can filter the rows in `features_df` for which $-\\log(\\text{df}_t / N) \\leq -\\log(0.01\\%)$ (i.e., the 'surprise' should be lower than $-\\log(0.01/100)$).","metadata":{"_uuid":"b79ad69b3b2b29c4e87abec0f3f3dd8ba0023e89"}},{"cell_type":"code","source":"# Approximately 180,000 reviews in the data set\n0.0001 * 180000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This shows that it requires at least 18 reviews per document in order to be included in the analysis if we were to take a boundary of 0.01%.","metadata":{}},{"cell_type":"code","source":"cut_off = -log(0.01/100)\nprint(cut_off)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:17:42.831999Z","iopub.execute_input":"2023-10-09T11:17:42.834133Z","iopub.status.idle":"2023-10-09T11:17:43.002154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a safeguard, we use a boxplot to visualize the distribution of \"surprise\" that the tokens come with. We should see that the vast majority of tokens should lie below the cutoff and remain included in the next step.","metadata":{}},{"cell_type":"code","source":"# boxplot of the inverse document frequency `idf` of the features_df\nboxplot_idf <- features_df %>%\n  ggplot(aes(x = 1, y = idf)) +  # Using x = 1 to create a single boxplot\n  geom_boxplot(fill = \"blue\") + \n  labs(title = \"Distribution of IDF (Inverse Document Frequency)\",\n       x = \"All reviews\", y = \"IDF (Inverse Document Frequency)\") + \n  theme(legend.position = \"none\") +\n  theme_minimal()\n\nboxplot_idf","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:17:45.696059Z","iopub.execute_input":"2023-10-09T11:17:45.698934Z","iopub.status.idle":"2023-10-09T11:17:51.854690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Keep only tokens that appeared in 18 reviews or more\nnrow(features_df)\nfeatures_df = features_df %>% filter(idf <= cut_off)\nnrow(features_df)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:17:55.254932Z","iopub.execute_input":"2023-10-09T11:17:55.256770Z","iopub.status.idle":"2023-10-09T11:17:55.519640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After excluding those tokens with a very low frequency, the distribution of \"surprise\" in the tokens looks as follows.","metadata":{}},{"cell_type":"code","source":"# boxplot of the inverse document frequency `idf` of the features_df\nboxplot_idf <- features_df %>%\n  ggplot(aes(x = 1, y = idf)) +  # Using x = 1 to create a single boxplot\n  geom_boxplot(fill = \"blue\") + \n  labs(title = \"Distribution of IDF (Inverse Document Frequency)\",\n       x = \"All reviews\", y = \"IDF (Inverse Document Frequency)\") + \n  theme(legend.position = \"none\") +\n  theme_minimal()\n\nboxplot_idf","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:18:03.620941Z","iopub.execute_input":"2023-10-09T11:18:03.622692Z","iopub.status.idle":"2023-10-09T11:18:08.744930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-nine\"></a>\n## 9 Sparse Matrices\n\nFor word counts from longer documents we get huge matrices. This causes problems because large matrices take a lot of computer memory and the computation speed will be quite low (Van de Meer et al., 2023). In order to avoid that we used sparse matrices. Sparse matrices only store non-zero elements (triplets: row, col, value). This is efficient because most entries in the text analytics matrices are equal to 0 anyways.","metadata":{}},{"cell_type":"code","source":"# Turn features into a sparse design matrix \nX = features_df %>% \n    cast_sparse(id, token, tf_idf) %>% \n    # Remove rows that do not belong to cases\n    .[!is.na(rownames(.)),]\n\n# Verify the result:\nX[1:8,20:25]\ncat(\"rows, columns: \", dim(X))","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:18:18.460853Z","iopub.execute_input":"2023-10-09T11:18:18.464554Z","iopub.status.idle":"2023-10-09T11:18:19.429838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look at the memory footprint\nprint(object.size(X), unit=\"Mb\")","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:18:22.871477Z","iopub.execute_input":"2023-10-09T11:18:22.873136Z","iopub.status.idle":"2023-10-09T11:18:22.894953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split design matrix into training and test portions\nX_train <- X[rownames(X) %in% amazon_merged[!is.na(amazon_merged$satisfied),]$id,]\nX_test  <- X[rownames(X) %in% amazon_merged[is.na(amazon_merged$satisfied),]$id,]","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:18:25.512887Z","iopub.execute_input":"2023-10-09T11:18:25.515148Z","iopub.status.idle":"2023-10-09T11:18:25.609300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target variable\ny_train <-\n    # Order of the cases in `X` may not be the same anymore as in `sampled_amazon`,\n    # hence we line up rownames in X with `id` in `sampled_amazon`\n    data.frame(id = rownames(X)) %>% \n    inner_join(sampled_amazon_train, by = \"id\") %>% \n\n    # Extract 'satisfied'\n    pull(satisfied) %>%\n    as.factor()","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:18:30.708919Z","iopub.execute_input":"2023-10-09T11:18:30.711005Z","iopub.status.idle":"2023-10-09T11:18:30.743444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-ten\"></a>\n## 10 Model Fitting\n\nSince we do not have any knowledge about which features are mostly predictive in customer satisfaction we need to apply a process of trial and error. Though, our data is big resulting in thousands of candidate features such that automation to balance flexibility and predictive performance of this process becomes essential.\n\nIn order to control the flexibility of the models, a tuning parameter lamda needs to be adjusted such that the optimal predictive performance of a model can be found. The optimal size of lambda can be determined by using either a validation set, or using cross-validation. If lambda is zero, then the tuning does not have any effects on the model, it will simply estimate least squares in case of linear models. Larger values of lambda lead to more coeffiecients being drawn towards zero, thus, being not included in the model. This results in a less flexible model, leading to lower variance and increased bias (James et al., 2021).\n\nThe type of regularization that is used by `glmnet` is controled by the `alpha` parameter.  The amount of regularization is specified by means of the `lambda` parameter. To tune the lambda parameter cross-validation was applied with help of the `cv.glmnet()` function (Van de Meer, 2023).\n\n\n<a id=\"subsection-ten-one\"></a>\n### 10.1 Shrinkage Methods\n\nShrinkage methods are techniques to shrink the regression coefficients towards zero. First, a model is fit that contains all predictors. Then, the coefficient estimates are constrained / regularized given the tuning parameter lambda to reduce the variance of the model (James et al., 2021). Models using shrinkage methods enable to take many features while automatically reducing redundant flexibility to any desired level.\n\n<a id=\"subsection-ten-one-one\"></a>\n#### 10.1.1 Ridge regression\nRidge regression shrinks coefficient estimates that do not contribute much to the prediction near zero. This penalty is not applied to the intercept (James et al., 2021).","metadata":{"_uuid":"5b5c79f14ac762022e5755d91264cbae0730b58b","trusted":true}},{"cell_type":"code","source":"# Speed up tuning by using all 4 CPU cores\ndoMC::registerDoMC(cores = 4)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:18:35.286971Z","iopub.execute_input":"2023-10-09T11:18:35.288954Z","iopub.status.idle":"2023-10-09T11:18:35.309143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross-validate the lambda parameter in ridge model\ncv_fitridge <- glmnet::cv.glmnet(\n    X_train,        # Training data (predictors)\n    y_train,        # Response variable\n    family = \"binomial\",    # Specify binary logistic regression\n    parallel = TRUE,        # Use parallel processing if possible\n    standardize = TRUE,     # Standardize the predictors\n    type.measure = \"auc\",   # Type of measure to optimize (Area Under the ROC Curve)\n    nfold = 5,              # Number of cross-validation folds\n    alpha = 0               # Ridge regression (vs. Lasso)\n)\n\n# View the cross-validated Ridge logistic regression model\ncv_fitridge","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:21:29.732799Z","iopub.execute_input":"2023-10-09T11:21:29.735355Z","iopub.status.idle":"2023-10-09T11:21:41.387119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimal value for the tuning parameter lambda\ncv_fitridge$lambda.min\nplot(cv_fitridge)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:21:44.952295Z","iopub.execute_input":"2023-10-09T11:21:44.954434Z","iopub.status.idle":"2023-10-09T11:21:45.066853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"subsection-ten-one-two\"></a>\n#### 10.1.2 Lasso\nLike ridge regression, the lasso forces a penalty on the coefficent estimates that do not contribute much to the prediction of the model so that they are drawn towards zero. But in contrast to ridge regression, the lasso shrinks some of the coefficient estimates, those that contribute the least to the prediction of the model, entirely to zero (James et al., 2021).","metadata":{}},{"cell_type":"code","source":"# Cross-validate the lambda parameter in lasso model\ncv_fitlasso <- glmnet::cv.glmnet(\n    X_train,        # Training data (predictors)\n    y_train,        # Response variable\n    family = \"binomial\",    # Specify binary logistic regression\n    parallel = TRUE,        # Use parallel processing if possible\n    standardize = TRUE,     # Standardize the predictors\n    type.measure = \"auc\",   # Type of measure to optimize (Area Under the ROC Curve)\n    nfold = 5,              # Number of cross-validation folds\n    alpha = 1               # Lasso regression\n)\n\n# View the cross-validated Lasso logistic regression model\ncv_fitlasso","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:18:41.825490Z","iopub.execute_input":"2023-10-09T11:18:41.827364Z","iopub.status.idle":"2023-10-09T11:18:47.913626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimal value for the tuning parameter lambda\ncv_fitlasso$lambda.min\nplot(cv_fitlasso)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:18:53.367448Z","iopub.execute_input":"2023-10-09T11:18:53.369514Z","iopub.status.idle":"2023-10-09T11:18:53.503346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"subsection-two\"></a>\n### 10.2 Dimension Reduction Methods\n\nDimension reduction methods transform predictors and fit a least squares model using the transformed variables. This way the regression problem gets reduced from estimating linear combinations for each predictor to estimating linear combinations for each dimensions (James et al., 2021).\n\n<a id=\"subsection-ten-two-one\"></a>\n#### 10.2.1 Principal Components Regression\n\nPrincipal components regression (PCR) is an unsupervised dimension reduction technique for regression where the first principal component is defined as the linear combination yielding the highest variance out of all possible linear combinations. It minimized the sum of squared distances between an observation datapoint and the regression line. The first principal component is chosen such that the regression line is as close as possible to the original observations. Thus, the first principal component will always contain the most information. The second principal component is a linear combination of variables that are unrelated to the variables of the first principal component. Thus, the second principal component is perpendicular / orthogonal to the first principal component. This principle is the same throughout all principal components. The number of princiap components can be chosen through cross-validation.\n\nThe assumption of PCR is that a small number of principal components suffices to explain variability in the data. In PCR the first principal components are constructed which are then used as predictors in a linear regression model (James et a., 2021).\n","metadata":{}},{"cell_type":"code","source":"# set.seed(1)\n# PCR with standardized predictors and cross-validation (10-fold cross-validation error for each possible number of principal components being used)\n# pcr_fit <- pcr(satisfied ~., data = amazon_merged, scale = TRUE, validation = \"CV\", parallel = TRUE)\n# summary(pcr_fit)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"subsection-ten-two-two\"></a>\n#### 10.2.2 Partial Least Sqaures\n\nPartial Least Squares (PLS) is a supervised alternative to PCR. \n\nWe ended up not includind the dimension reduction techniques, because we are only allowed using the glmnet and pls packages, the former not supporting PCR/PLS and the latter not supporting sparse matrix formats, which made it difficult for us to implement dimensionality reduction methods.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-eleven\"></a>\n## 11 Model Evaluation\n\nVarious predictive performance measures exist in order to evaluate the model.\nThe performance of our submission is evaluated using the ***area under the curve*** (**AUC**) of the *receiver operating curve* (*ROC*).\n\nOther indicators that were used are prediction accuracy, AIC, BIC, and adjusted R square.\n\n### 11.1 Performance of ridge regression\n\nFirst, we evaluated the performance of ridge regression on the training dataset in regards to prediction accuracy. ","metadata":{}},{"cell_type":"code","source":"# prediction accurary ridge regression\npred_ridge <- predict(\n    cv_fitridge,             # Cross-validated Ridge logistic regression model\n    X_train,                 # The training data (predictors)\n    s = \"lambda.min\",        # Use the lambda value that minimizes the cross-validated error\n    type = \"class\"           # Predict class labels\n) %>% \nas.factor()                 # Convert the predictions to a factor (categorical variable)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:21:53.735522Z","iopub.execute_input":"2023-10-09T11:21:53.737623Z","iopub.status.idle":"2023-10-09T11:21:53.789308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix ridge regression\nconfusion_cv_ridge <- caret::confusionMatrix(pred_ridge, y_train)\n\n# Accuracy\nmean(pred_ridge == y_train)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:21:56.612812Z","iopub.execute_input":"2023-10-09T11:21:56.614736Z","iopub.status.idle":"2023-10-09T11:21:56.643323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 11.2 Performance of lasso\n\nSecond, we evaluated the performance of lasso on the training dataset in regards to prediction accuracy. ","metadata":{}},{"cell_type":"code","source":"# prediction accurary lasso\npred_lasso <- predict(\n    cv_fitlasso,             # Cross-validated Lasso logistic regression model\n    X_train,                 # The training data (predictors)\n    s = 'lambda.min',        # Use the lambda value that minimizes the cross-validated error\n    type = 'class'           # Predict class labels\n) %>% \nas.factor()                 # Convert the predictions to a factor (categorical variable)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:20:00.885154Z","iopub.execute_input":"2023-10-09T11:20:00.893714Z","iopub.status.idle":"2023-10-09T11:20:01.017263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix lasso\nconfusion_cv_lasso <- caret::confusionMatrix(pred_lasso, y_train)\n\n# Accuracy\nmean(pred_lasso == y_train)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:20:03.897418Z","iopub.execute_input":"2023-10-09T11:20:03.900311Z","iopub.status.idle":"2023-10-09T11:20:05.779631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-twelve\"></a>\n## 12 Model Comparison","metadata":{}},{"cell_type":"markdown","source":"We compared the fitted models, the ridge regression model and the lasso, with different performance evaluation techniques on the training data. This way we could determine which model to use for the final predictions on the test set.","metadata":{}},{"cell_type":"code","source":"# Ridge\nacc_cv_ridge  <- mean(pred_ridge == y_train)                  # Accuracy\nauc_cv_ridge  <- max(cv_fitridge$cvm)                         # highest AUC value\nsens_cv_ridge <- confusion_cv_ridge$byClass[[\"Sensitivity\"]]  # Sensitivity\nspec_cv_ridge <- confusion_cv_ridge$byClass[[\"Specificity\"]]  # Specificity\n\n# Lasso\nacc_cv_lasso  <- mean(pred_lasso == y_train)                  # Accuracy\nauc_cv_lasso  <- max(cv_fitlasso$cvm)                         # highest AUC value\nsens_cv_lasso <- confusion_cv_lasso$byClass[[\"Sensitivity\"]]  # Sensitivity\nspec_cv_lasso <- confusion_cv_lasso$byClass[[\"Specificity\"]]  # Specificity\n\n# Table to see which model has highest accuracy, highest AUC value, Sensitity and Specificity\ntable <- data.frame(\n  Model = c(\"Lasso\", \"Ridge\"),\n  AUC = c(auc_cv_lasso, auc_cv_ridge),\n  Accuracy = c(acc_cv_lasso, acc_cv_ridge),\n  Sensitivity = c(sens_cv_lasso, sens_cv_ridge),\n  Specificity = c(spec_cv_lasso, spec_cv_ridge)\n)\ntable\n\n\n# Plot 1: AUC\nplot_auc <- ggplot(table, aes(x = Model, y = AUC, fill = Model)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"AUC Comparison\",\n       x = \"Model\",\n       y = \"AUC\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n# Plot 2: Accuracy\nplot_accuracy <- ggplot(table, aes(x = Model, y = Accuracy, fill = Model)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Accuracy Comparison\",\n       x = \"Model\",\n       y = \"Accuracy\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_brewer(palette = \"Set2\")\n\n# Plot 3: Sensitivity\nplot_sensitivity <- ggplot(table, aes(x = Model, y = Sensitivity, fill = Model)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Sensitivity Comparison\",\n       x = \"Model\",\n       y = \"Sensitivity\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_brewer(palette = \"Set2\")\n\n# Plot 4: Specificity\nplot_specificity <- ggplot(table, aes(x = Model, y = Specificity, fill = Model)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Specificity Comparison\",\n       x = \"Model\",\n       y = \"Specificity\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\ngridExtra::grid.arrange(plot_auc, plot_accuracy, plot_sensitivity, plot_specificity, nrow = 2)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:22:03.012522Z","iopub.execute_input":"2023-10-09T11:22:03.014391Z","iopub.status.idle":"2023-10-09T11:22:04.604792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-thirteen\"></a>\n## 13 Predictions on Test Data","metadata":{}},{"cell_type":"markdown","source":"Based on our model we made predictions on the test data. Those predictions are submitted in the competition.","metadata":{}},{"cell_type":"code","source":"# Performance on test set\nPred_lasso = predict(\ncv_fitlasso, X_test, s = 'lambda.min', type = 'response'\n) %>% factor()","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:22:19.831481Z","iopub.execute_input":"2023-10-09T11:22:19.833608Z","iopub.status.idle":"2023-10-09T11:22:19.976991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-fourteen\"></a>\n## 14 Submission\n\nIn order to participate in the competition, the predictions had to be formatted and saved in a csv. file. The csv. file was submitted to the competition.","metadata":{"_uuid":"db4164fa915f86b512ba3a4a576683dcf4b48320","trusted":true}},{"cell_type":"code","source":"# format predictions\npredictions_df <- data.frame(\n    Id = rownames(X_test),\n    Prediction = Pred_lasso\n)\npredictions_df <- predictions_df %>% arrange(as.integer(Id))\n\nhead(predictions_df)","metadata":{"_uuid":"df586ac1a39965a4426b6a51d4e3c35de25d2255","execution":{"iopub.status.busy":"2023-10-09T11:22:25.602857Z","iopub.execute_input":"2023-10-09T11:22:25.605332Z","iopub.status.idle":"2023-10-09T11:22:25.664511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write predictions data frame to file\nwrite_csv(predictions_df, file=\"predictions.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-09T11:22:28.987694Z","iopub.execute_input":"2023-10-09T11:22:28.989962Z","iopub.status.idle":"2023-10-09T11:22:29.040244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-fifteen\"></a>\n## 15 Discussion\n\nTask of this competition was to predict customer satisfaction based on their written reviews. Given that customer satisfaction is determined by a five-star system on amazon we were faced with a classification problem. \n\nModels we fitted on the training data included a ridge regression model and the lasso. Attempts to fit principal component regression models or a partial least squares model failed because the R function for fitting PCR and PLS, respectively do not allow for sparse matrices. However, fitting the PCR model and the PLS model to the vast amount of data we had would have been computationally expensive. \n\nOur first submission contained a simple lasso with only Term frequency *TF*, Inverse document frequency *IDF*, and Product *TFIDF* (Version 21). This resulted in a quite good prediction score of 0.81476.\n\nWe worked on features and a lasso regression model containing only unigrams which gave a predictive value of 0.93292.\n\nSo far, we have only tried lasso models, as they always had a better AUC value and specificity, but since the ridge model had better specificity (Lasso: 0.9607033; Ridge: 0.9769469) and almost similar accuracy (Lasso: 0.8990497, Ridge: 0.8931877), we were curious to try a submission with a ridge regression model as well. This resulted in a slightly worse prediction of 0.92579, whereas the previous version with Lasso had a value of 0.93292. Since our suspicion that Ridge regression might be a better model was refuted, we stuck with the Lasso model for subsequent runs. Furthermore, we hypothesize that the lasso model deals overfitting issues better the more features we add because coefficients that do not contribute much to the predictions are drawn to zero (James, et al., 2021). \n\nThe submission of the first round contained a lasso regression model, where we used a bigram only. This resulted in a prediction score of 0.94951 meaning that the prediction score improved further.\n\nFor the second round of the competition we focused on sentiment analysis under the assumption that the sentiment of a review can also be a good indicator of a satisfied vs. unsatisfied customers based on their written review texts. Thus, we used unigrams conducted sentiment analyses. We used the NRC library for that. The NRC assigns positive or negative valences or one of the 8 basic emotions to the token. Given that we were to predict customer satisfaction (satisfied - unsatisfied) we also used the Bing library which assigns 'positive' or 'negative' to tokens. This way we could predict the positive vs. negative sentiment of a review (Class 3: Text Analytics - Big 5 from Text). Eventually, the sentiment analysis did not work due to problems with the length of the prediction file. \n\nThus, for the final prediction our best AUC scores were achieved using bigrams with the selected features as described above.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-sixteen\"></a>\n## 16 References\n\n​\nHe, R. & McAuley, J.(2016). Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. Amazon Product Data. WWW. Retreived from: http://jmcauley.ucsd.edu/data/amazon/\n​\n\ndan_vdmeer, Dave Leitritz, Joost van Kordelaar, Raoul. (2023). BDA 2023 Customer sentiment from reviews. Kaggle. https://kaggle.com/competitions/customer-sentiment-from-reviews-bda-2023\n\n​\nGrasman, R. (2018). Feature extraction from Signals. DropBox. Retrieved from: https://paper.dropbox.com/doc/Feature-extraction-from-Signals-qCp5uvj47gmyuw5nmB8lL\n\n​\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An introduction to statistical learning (1st ed.). Springer.\n\n​\nHaque, T. U., Saber, N. N., & Shah, F. M. (2018, May). Sentiment analysis on large scale Amazon product reviews. In 2018 IEEE international conference on innovative research and development (ICIRD) (pp. 1-6). IEEE.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-seventeen\"></a>\n## 17 Task Division\n​\n* **Vincent:** TFIDF, sparse matrix, bayes´ error bound, amazon_merged pipeline\n* **Lisa:** Features, implementing models, comparisons\n* **Sophia:** Features, writing and formatting\n","metadata":{}}]}